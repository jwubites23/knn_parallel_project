---
title: "Brute force KNN parallelisation experiment"
author: "Marc van Heerden (1769865)"
date: "3/8/2018"
output:
  github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Overview

K nearest neighbours is a commonly used algorithm that can be used for regression or classification. For this experiment we use the regression version of the algorithm. For a given problem there are n reference points and q query points. For a given integer k, the estimated value for a query point is given as the average value of the k nearest reference points.

There are various ways to compute the estimated values, in this report we attempt to improve the brute force method with parallelisation of the distance calculation and sorting calculation. 

## Parallelisation

### Foster's methodology

The distance calculation involves calculating the distances from each of the query points to each of the reference points. We partition this problem into the fine-grained task of calculating the distance from one query point to all the reference points. There is no communication needed between the partitions. We agglomerate these tasks into roughly equal groups, the number of groups is equal to the number of available processors. 

The sorting calculation involves sorting distance vectors that result from the distance calculation in order to find the k closest points to each query point. We partition this problem into the fine-grained task of sorting one query point's distance vector. There is no communication needed between the partitions. Once again we agglomerate these tasks into roughly equal groups, the number of groups is equal to the number of available processors.


### Results

```{r cars}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(knitr))

exp_data <- suppressMessages(readr::read_csv("exp20180308.csv"))

median_data <- 
    exp_data %>%
    mutate(time = time_dist + time_sort) %>%
    group_by(m,n, distance_function, sort_function, serial_parallel) %>%
    summarise(time = median(time),
              time_dist = median(time_dist),
              time_sort = median(time_sort)) %>%
    ungroup
```

The experiment was performed with multiple sorting functions and distance functions. For this section we analyse the results from the quicksort algorithm and euclidean distance. Table 1 and 2 below show the proportion of the run time spent on the distance calculation and on the sorting calculation. It can be seen the the proportion of time spent on calculating the distance decreases as n increases. This is due to the higher complexity of the sorting algorithm. It can also be seen that the distance calculation has a higher proportion of the total time for the parallel algorithm compared to the serial algorithm.


```{r}

median_data %>%
    filter(sort_function == "quick",
           distance_function == "euclidean",
           m == 450L,
           serial_parallel == "serial") %>%
    mutate(distance_time_proportion = round(time_dist/time,3),
           sorting_time_proportion = round(time_sort/time,3),
           total_time_seconds = round(time,3)) %>%
    select(n, distance_time_proportion, sorting_time_proportion, total_time_seconds) %>%
    kable(caption = "Run times for the serial algorithm, m = 450")

median_data %>%
    filter(sort_function == "quick",
           distance_function == "euclidean",
           m == 450L,
           serial_parallel == "parallel") %>%
    mutate(distance_time_proportion = round(time_dist/time,3),
           sorting_time_proportion = round(time_sort/time,3),
           total_time_seconds = round(time,3)) %>%
    select(n, distance_time_proportion, sorting_time_proportion, total_time_seconds) %>%
    kable(caption = "Run times for the parallel algorithm m = 450")


```

We define the speed up factor as the serial runtime divided by the parallel runtime. Table 3 below compares the serial and parallel runtimes for m = 450 and various n values. It can be seen that the speed up factor increase as n increases.
```{r}

median_data %>%
    filter(sort_function == "quick",
           distance_function == "euclidean",
           m == 450L) %>%
    select(n, time, serial_parallel) %>%
    spread(serial_parallel, time) %>%
    mutate(speed_up_factor = round(serial/parallel,2)) %>%
    kable(caption = "Parallel/serial runtime comparison for m = 450")


```

Figure 1 shows the comparative run times of the serial and parallel algorithms for various values of m and n. The y-axis uses a log base 10 scale due to the vastly different scales of the parallel and serial algorithms. It can be seen that parallelisation consistently and significantly improves the run times.
```{r}

median_data %>%
    filter(sort_function == "quick",
           distance_function == "euclidean") %>%
    ggplot(aes(n, time, color = serial_parallel)) +
        geom_point() +
        facet_grid(m~.) +
        scale_y_log10(name = "time in seconds (log10 scale)", 
                      breaks = c(0.03, 0.1, 0.3, 1, 3)) + 
        ggtitle("Fig1 : Log scale comparison or parallel and serial runtimes") + 
        theme_bw() + 
        theme(text=element_text(size=10,  family="serif"))

```

## Sorting algorithms

Insertion sort was significantly slower than mergesort and quicksort. Consequently, insertion sort was run on n values that were 5% of the n values run for mergesort and quicksort.

```{r}

median_data %>%
    filter(m == 450,
           distance_function == "euclidean") %>%
    ggplot(aes(n, time_sort, color = serial_parallel)) +
        geom_point() + 
        facet_grid(sort_function~.) +
        scale_x_log10(name = "n (log10 scale)",
                      breaks = c(1000, 3000, 10000, 30000)) +
        scale_y_continuous(name = "sorting time") +
        ggtitle("Fig 2 : Sorting time per sorting algorithm, m = 450") + 
        theme_bw() + 
        theme(text=element_text(size=10,  family="serif"))

```


Quicksort was faster than merge sort for all n and in the serial and parallel runs.


```{r}

median_data %>%
    filter(m == 450,
           distance_function == "euclidean",
           sort_function != "insertion") %>%
    rename(sorting_time_sec = time_sort) %>%
    mutate(n = paste0("n=", n)) %>%
    select(n, sorting_time_sec, sort_function, serial_parallel) %>% 
    spread(n, sorting_time_sec) %>%
    kable(caption = "Sorting times in seconds per sorting function and serial/parallel. m = 450")

```

## System specifications

```{r}
cpu_specs <- 
    system("lscpu", intern=TRUE)[c(1,4,13)] %>%
    str_split(":") %>%
    lapply(str_trim)

cpu_specs_vect <- map_chr(cpu_specs, 2)
names(cpu_specs_vect) <- tolower(map_chr(cpu_specs, 1))

kable(c(Sys.info()[1:3], cpu_specs_vect), col.names="")

```

